{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature Reduction with PCA (Principal Component Analysis)\n",
    "\n",
    "## Objective\n",
    "Feature Reduction, also known as dimension reduction, involves transforming data from a high-dimensional space to a low-dimensional space, ensuring the new representation retains meaningful properties of the original data. **Principal Component Analysis (PCA)**, introduced by Karl Pearson in 1901, is a widely used technique for this purpose. PCA performs an orthogonal transformation to convert a dataset of possibly correlated features into a set of linearly uncorrelated variables called principal components, with fewer components than the original features. This process is sensitive to the relative scaling of the original features. The aim of this lab is to apply PCA for feature reduction.\n",
    "\n",
    "## Prerequisites\n",
    "Complete all materials in submodule 3.6, specifically the lecture slides on PCA, to understand the underlying algorithm and its application.\n",
    "\n",
    "## PCA Algorithm\n",
    "To implement PCA, follow these steps:\n",
    "\n",
    "1. **Centralize the features** by subtracting the mean of each feature from the dataset.\n",
    "2. **Calculate the covariance matrix** of the centered features to understand how features vary together.\n",
    "3. **Perform eigendecomposition** on the covariance matrix to extract eigenvalues and eigenvectors.\n",
    "4. **Sort eigenvalues and eigenvectors** in descending order based on the eigenvalues, which represent the variance captured by each eigenvector.\n",
    "5. **Select the top K eigenvectors** based on the sorted eigenvalues, where K is the number of principal components you wish to retain.\n",
    "6. **Construct the principal components (PCs)** using the selected eigenvectors, which will be the new, lower-dimensional representation of your data.\n",
    "\n",
    "## Instructions\n",
    "- Implement the PCA algorithm using Python.\n",
    "- Apply PCA to the **IRIS dataset** by selecting the first two principal components for feature reduction. The dataset is available `iris.data` or at [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).\n",
    "- **Plot the data** using the two principal components to visualize the reduced dimensionality.\n",
    "\n",
    "This exercise will help you gain hands-on experience with PCA and understand how it can be applied to reduce feature dimensions while preserving the essence of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.534351100Z",
     "start_time": "2024-02-03T13:06:28.189482300Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-learn \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.560773100Z",
     "start_time": "2024-02-03T13:06:28.506744Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "|details-start|\n",
      "**References**\n",
      "|details-split|\n",
      "\n",
      "- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "  Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "  Structure and Classification Rule for Recognition in Partially Exposed\n",
      "  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "  on Information Theory, May 1972, 431-433.\n",
      "- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "  conceptual clustering system finds 3 classes in the data.\n",
      "- Many, many more ...\n",
      "\n",
      "|details-end|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the length and the width of the sepals and petals,\n",
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note iris.target has the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.598992200Z",
     "start_time": "2024-02-03T13:06:28.531324300Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iris.data has the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.703982800Z",
     "start_time": "2024-02-03T13:06:28.542740500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [4.4, 2.9, 1.4, 0.2]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = iris.data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the data, to remove bias due to differences in feature scales\n",
    "Call StandardScaler().fit_transform\\\n",
    "How would this be computed by hand?\\\n",
    "Why is standarization of the data important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.705616200Z",
     "start_time": "2024-02-03T13:06:28.616114300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90068117,  1.01900435, -1.34022653, -1.3154443 ],\n",
       "       [-1.14301691, -0.13197948, -1.34022653, -1.3154443 ],\n",
       "       [-1.38535265,  0.32841405, -1.39706395, -1.3154443 ],\n",
       "       [-1.50652052,  0.09821729, -1.2833891 , -1.3154443 ],\n",
       "       [-1.02184904,  1.24920112, -1.34022653, -1.3154443 ],\n",
       "       [-0.53717756,  1.93979142, -1.16971425, -1.05217993],\n",
       "       [-1.50652052,  0.78880759, -1.34022653, -1.18381211],\n",
       "       [-1.02184904,  0.78880759, -1.2833891 , -1.3154443 ],\n",
       "       [-1.74885626, -0.36217625, -1.34022653, -1.3154443 ],\n",
       "       [-1.14301691,  0.09821729, -1.2833891 , -1.44707648]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code here\n",
    "std_data = StandardScaler().fit_transform(data)\n",
    "std_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasnpose to capture all the values for each feature into one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.747299600Z",
     "start_time": "2024-02-03T13:06:28.621251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.90068117, -1.14301691, -1.38535265, -1.50652052, -1.02184904,\n",
       "       -0.53717756, -1.50652052, -1.02184904, -1.74885626, -1.14301691,\n",
       "       -0.53717756, -1.26418478, -1.26418478, -1.87002413, -0.05250608,\n",
       "       -0.17367395, -0.53717756, -0.90068117, -0.17367395, -0.90068117,\n",
       "       -0.53717756, -0.90068117, -1.50652052, -0.90068117, -1.26418478,\n",
       "       -1.02184904, -1.02184904, -0.7795133 , -0.7795133 , -1.38535265,\n",
       "       -1.26418478, -0.53717756, -0.7795133 , -0.41600969, -1.14301691,\n",
       "       -1.02184904, -0.41600969, -1.14301691, -1.74885626, -0.90068117,\n",
       "       -1.02184904, -1.62768839, -1.74885626, -1.02184904, -0.90068117,\n",
       "       -1.26418478, -0.90068117, -1.50652052, -0.65834543, -1.02184904,\n",
       "        1.40150837,  0.67450115,  1.2803405 , -0.41600969,  0.79566902,\n",
       "       -0.17367395,  0.55333328, -1.14301691,  0.91683689, -0.7795133 ,\n",
       "       -1.02184904,  0.06866179,  0.18982966,  0.31099753, -0.29484182,\n",
       "        1.03800476, -0.29484182, -0.05250608,  0.4321654 , -0.29484182,\n",
       "        0.06866179,  0.31099753,  0.55333328,  0.31099753,  0.67450115,\n",
       "        0.91683689,  1.15917263,  1.03800476,  0.18982966, -0.17367395,\n",
       "       -0.41600969, -0.41600969, -0.05250608,  0.18982966, -0.53717756,\n",
       "        0.18982966,  1.03800476,  0.55333328, -0.29484182, -0.41600969,\n",
       "       -0.41600969,  0.31099753, -0.05250608, -1.02184904, -0.29484182,\n",
       "       -0.17367395, -0.17367395,  0.4321654 , -0.90068117, -0.17367395,\n",
       "        0.55333328, -0.05250608,  1.52267624,  0.55333328,  0.79566902,\n",
       "        2.12851559, -1.14301691,  1.76501198,  1.03800476,  1.64384411,\n",
       "        0.79566902,  0.67450115,  1.15917263, -0.17367395, -0.05250608,\n",
       "        0.67450115,  0.79566902,  2.24968346,  2.24968346,  0.18982966,\n",
       "        1.2803405 , -0.29484182,  2.24968346,  0.55333328,  1.03800476,\n",
       "        1.64384411,  0.4321654 ,  0.31099753,  0.67450115,  1.64384411,\n",
       "        1.88617985,  2.4920192 ,  0.67450115,  0.55333328,  0.31099753,\n",
       "        2.24968346,  0.55333328,  0.67450115,  0.18982966,  1.2803405 ,\n",
       "        1.03800476,  1.2803405 , -0.05250608,  1.15917263,  1.03800476,\n",
       "        1.03800476,  0.55333328,  0.79566902,  0.4321654 ,  0.06866179])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code here\n",
    "data_T = std_data.T\n",
    "data_T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcuate the covariance matrix\n",
    "Use the numpy .cov funciton\\\n",
    "What does this matrix tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.785746500Z",
     "start_time": "2024-02-03T13:06:28.629697800Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate eigen values and vectors\n",
    "Use the np.linalg.eig function to extract the eigen values vecotrs of the covariance matrix.\\\n",
    "This step could have been done with SVD as well\\\n",
    "Repeat lab using SVD instead for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.970057400Z",
     "start_time": "2024-02-03T13:06:28.633217500Z"
    }
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the eigen vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.972055Z",
     "start_time": "2024-02-03T13:06:28.640311700Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the eigen values\\\n",
    "What do the eigen values tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.974054600Z",
     "start_time": "2024-02-03T13:06:28.647456900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the importance of each feature\n",
    "Which eigen vectors do we keep and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:28.997645500Z",
     "start_time": "2024-02-03T13:06:28.652524200Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project the data onto the first basis (i.e., the 'axis' of the most improtant eigen vector)\n",
    "That is take the dot product of the orignal data, with the most important eigen vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.000678300Z",
     "start_time": "2024-02-03T13:06:28.658058900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse into pandas dataframe\\\n",
    "The y-axis is set to 0 so can use 2D graph to display one dimentional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.001643100Z",
     "start_time": "2024-02-03T13:06:28.662642700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualzie with scatter plot\\\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.026330300Z",
     "start_time": "2024-02-03T13:06:28.670152400Z"
    }
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat with 2nd most important principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project onto the second basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.028343900Z",
     "start_time": "2024-02-03T13:06:28.675657600Z"
    }
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.053965800Z",
     "start_time": "2024-02-03T13:06:28.681813800Z"
    }
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 2D graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.055636900Z",
     "start_time": "2024-02-03T13:06:28.688916900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does these plots compare to projecting onto PC3 or some 'axis' that captures less variance?\\\n",
    "Project the data onto the third basis\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.066715800Z",
     "start_time": "2024-02-03T13:06:28.693463900Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate plot with the third basis only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.076785400Z",
     "start_time": "2024-02-03T13:06:28.710533100Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can also use the PCA from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.209469300Z",
     "start_time": "2024-02-03T13:06:28.727869100Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call PCA with the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.210469400Z",
     "start_time": "2024-02-03T13:06:29.125466200Z"
    }
   },
   "outputs": [],
   "source": [
    " # code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Parse into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.211475300Z",
     "start_time": "2024-02-03T13:06:29.129510Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.213445200Z",
     "start_time": "2024-02-03T13:06:29.134164400Z"
    }
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T13:06:29.215977900Z",
     "start_time": "2024-02-03T13:06:29.140734400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
